{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c58d3417",
   "metadata": {},
   "source": [
    "코퍼스(corpus)(말뭉치)는 정형이나 비정형인 단어나 표현의 묶음이다. 모든 NLTK 코퍼스는 nltk.corpus 모듈에 저장되어 있다. 예를 들면 다음과 같다.\n",
    "\n",
    " \n",
    "\n",
    "◼︎ gutenberg : <모비딕(Moby Dick)>이나 <성경> 등 구텐베르크 프로젝트(Gutenberg Project)에서 제공하는 영문 텍스트 18개\n",
    "\n",
    "◼︎ names : 8000개의 남성과 여성의 이름 리스트\n",
    "\n",
    "◼︎ words : 가장 빈번하게 사용하는 영어 단어 23만 5000개\n",
    "\n",
    "◼︎ stopwords : 14개의 언어로 된 가장 많이 사용하는 불용어(stop word) 리스트. 영어로 된 리스트는 stop words.words(\"english\")에 저장되어 있다. 불용어는 대부분의 분석에서 보통 삭제하는데, 텍스트 이해에 별로 기여하는 바가 없기 때문이다.\n",
    "\n",
    "◼︎ cmudict : 카네기멜론대학교에서 만든 발음 사전으로 13만 4000개 입력 데이터가 있다. cmudict.entries()의 각 입력 데이터는 단어와 그 음절(syllables) 리스트의 튜플이다. 단어가 같더라도 다르게 발음할 수 있다. 이 코퍼스를 사용하면 발음이 같은 동음이의어(homophones)를 찾아볼 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5559548",
   "metadata": {},
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(len(stopwords.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6965de57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236736\n",
      "7944\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import words\n",
    "print(len(words.words()))\n",
    "from nltk.corpus import names\n",
    "print(len(names.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c00cc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [59:50<00:00, 11.14it/s]   \n"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    "dic8 = dict()\n",
    "for search_query in tqdm(words.words()[160000:200000]):\n",
    "    url = (f'http://aha-dic.com/View.asp?word={search_query}')\n",
    "    res = requests.get(url) \n",
    "    soup = BeautifulSoup(res.text, \"lxml\")\n",
    "    \n",
    "    try:\n",
    "        dic8[search_query] = soup.find_all(\"span\", {\"class\": \"phoneticKor\"})[0].text.replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "    except:\n",
    "        dic8[search_query] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb12c660",
   "metadata": {},
   "source": [
    "import pickle\n",
    "with open(\"dic5.pkl\",\"wb\") as f:\n",
    "    pickle.dump(dic,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6491120",
   "metadata": {},
   "source": [
    "import pickle\n",
    "with open(\"dic1_5.pkl\",\"wb\") as f:\n",
    "    pickle.dump(dic8,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def999fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TTS",
   "language": "python",
   "name": "tts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
